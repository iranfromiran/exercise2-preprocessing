{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORkNvNmhakEXfkoTb+yvQ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iranfromiran/exercise2-preprocessing/blob/main/nlp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9Geld76zdl5",
        "outputId": "93d2314c-0a99-46d5-b388-d7a8c5b672a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"Given a sound clip of a person or people speaking, determine the textual representation of the speech. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \"AI-complete\" (see above). In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process. Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent.Tokenization is a process used in text analysis that divides text into individual words or word fragments. This technique results in two key components: a word index and tokenized text. The word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token. These numerical tokens are then used in various deep learning methods.[24]\n",
        "For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Sometimes this process is also used in cases like bag of words (BOW) creation in data mining. Morphological segmentation, Separate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., \"open, opens, opened, opening\") as separate words. In languages such as Turkish or Meitei, a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms. Lemmatization, The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the transformation actually uses a dictionary to map words to their actual form.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VTEWm0AKz9CB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgTBKhTK1TwF",
        "outputId": "9986571d-3d10-4ac6-e53f-7ea8cd92a9b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Given a sound clip of a person or people speaking, determine the textual representation of the speech.',\n",
              " 'This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \"AI-complete\" (see above).',\n",
              " 'In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below).',\n",
              " 'In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process.',\n",
              " 'Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent.Tokenization is a process used in text analysis that divides text into individual words or word fragments.',\n",
              " 'This technique results in two key components: a word index and tokenized text.',\n",
              " 'The word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token.',\n",
              " 'These numerical tokens are then used in various deep learning methods.',\n",
              " '[24]\\nFor a language like English, this is fairly trivial, since words are usually separated by spaces.',\n",
              " 'However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language.',\n",
              " 'Sometimes this process is also used in cases like bag of words (BOW) creation in data mining.',\n",
              " 'Morphological segmentation, Separate words into individual morphemes and identify the class of the morphemes.',\n",
              " 'The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered.',\n",
              " 'English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., \"open, opens, opened, opening\") as separate words.',\n",
              " 'In languages such as Turkish or Meitei, a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.',\n",
              " 'Lemmatization, The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma.',\n",
              " 'Lemmatization is another technique for reducing words to their normalized form.',\n",
              " 'But in this case, the transformation actually uses a dictionary to map words to their actual form.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(paragraph)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ftJQIEH2EYM",
        "outputId": "bb9ad6dc-0128-4b2c-fd01-c93c7e5d4df1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Given',\n",
              " 'a',\n",
              " 'sound',\n",
              " 'clip',\n",
              " 'of',\n",
              " 'a',\n",
              " 'person',\n",
              " 'or',\n",
              " 'people',\n",
              " 'speaking',\n",
              " ',',\n",
              " 'determine',\n",
              " 'the',\n",
              " 'textual',\n",
              " 'representation',\n",
              " 'of',\n",
              " 'the',\n",
              " 'speech',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'the',\n",
              " 'opposite',\n",
              " 'of',\n",
              " 'text',\n",
              " 'to',\n",
              " 'speech',\n",
              " 'and',\n",
              " 'is',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'extremely',\n",
              " 'difficult',\n",
              " 'problems',\n",
              " 'colloquially',\n",
              " 'termed',\n",
              " '``',\n",
              " 'AI-complete',\n",
              " \"''\",\n",
              " '(',\n",
              " 'see',\n",
              " 'above',\n",
              " ')',\n",
              " '.',\n",
              " 'In',\n",
              " 'natural',\n",
              " 'speech',\n",
              " 'there',\n",
              " 'are',\n",
              " 'hardly',\n",
              " 'any',\n",
              " 'pauses',\n",
              " 'between',\n",
              " 'successive',\n",
              " 'words',\n",
              " ',',\n",
              " 'and',\n",
              " 'thus',\n",
              " 'speech',\n",
              " 'segmentation',\n",
              " 'is',\n",
              " 'a',\n",
              " 'necessary',\n",
              " 'subtask',\n",
              " 'of',\n",
              " 'speech',\n",
              " 'recognition',\n",
              " '(',\n",
              " 'see',\n",
              " 'below',\n",
              " ')',\n",
              " '.',\n",
              " 'In',\n",
              " 'most',\n",
              " 'spoken',\n",
              " 'languages',\n",
              " ',',\n",
              " 'the',\n",
              " 'sounds',\n",
              " 'representing',\n",
              " 'successive',\n",
              " 'letters',\n",
              " 'blend',\n",
              " 'into',\n",
              " 'each',\n",
              " 'other',\n",
              " 'in',\n",
              " 'a',\n",
              " 'process',\n",
              " 'termed',\n",
              " 'coarticulation',\n",
              " ',',\n",
              " 'so',\n",
              " 'the',\n",
              " 'conversion',\n",
              " 'of',\n",
              " 'the',\n",
              " 'analog',\n",
              " 'signal',\n",
              " 'to',\n",
              " 'discrete',\n",
              " 'characters',\n",
              " 'can',\n",
              " 'be',\n",
              " 'a',\n",
              " 'very',\n",
              " 'difficult',\n",
              " 'process',\n",
              " '.',\n",
              " 'Also',\n",
              " ',',\n",
              " 'given',\n",
              " 'that',\n",
              " 'words',\n",
              " 'in',\n",
              " 'the',\n",
              " 'same',\n",
              " 'language',\n",
              " 'are',\n",
              " 'spoken',\n",
              " 'by',\n",
              " 'people',\n",
              " 'with',\n",
              " 'different',\n",
              " 'accents',\n",
              " ',',\n",
              " 'the',\n",
              " 'speech',\n",
              " 'recognition',\n",
              " 'software',\n",
              " 'must',\n",
              " 'be',\n",
              " 'able',\n",
              " 'to',\n",
              " 'recognize',\n",
              " 'the',\n",
              " 'wide',\n",
              " 'variety',\n",
              " 'of',\n",
              " 'input',\n",
              " 'as',\n",
              " 'being',\n",
              " 'identical',\n",
              " 'to',\n",
              " 'each',\n",
              " 'other',\n",
              " 'in',\n",
              " 'terms',\n",
              " 'of',\n",
              " 'its',\n",
              " 'textual',\n",
              " 'equivalent.Tokenization',\n",
              " 'is',\n",
              " 'a',\n",
              " 'process',\n",
              " 'used',\n",
              " 'in',\n",
              " 'text',\n",
              " 'analysis',\n",
              " 'that',\n",
              " 'divides',\n",
              " 'text',\n",
              " 'into',\n",
              " 'individual',\n",
              " 'words',\n",
              " 'or',\n",
              " 'word',\n",
              " 'fragments',\n",
              " '.',\n",
              " 'This',\n",
              " 'technique',\n",
              " 'results',\n",
              " 'in',\n",
              " 'two',\n",
              " 'key',\n",
              " 'components',\n",
              " ':',\n",
              " 'a',\n",
              " 'word',\n",
              " 'index',\n",
              " 'and',\n",
              " 'tokenized',\n",
              " 'text',\n",
              " '.',\n",
              " 'The',\n",
              " 'word',\n",
              " 'index',\n",
              " 'is',\n",
              " 'a',\n",
              " 'list',\n",
              " 'that',\n",
              " 'maps',\n",
              " 'unique',\n",
              " 'words',\n",
              " 'to',\n",
              " 'specific',\n",
              " 'numerical',\n",
              " 'identifiers',\n",
              " ',',\n",
              " 'and',\n",
              " 'the',\n",
              " 'tokenized',\n",
              " 'text',\n",
              " 'replaces',\n",
              " 'each',\n",
              " 'word',\n",
              " 'with',\n",
              " 'its',\n",
              " 'corresponding',\n",
              " 'numerical',\n",
              " 'token',\n",
              " '.',\n",
              " 'These',\n",
              " 'numerical',\n",
              " 'tokens',\n",
              " 'are',\n",
              " 'then',\n",
              " 'used',\n",
              " 'in',\n",
              " 'various',\n",
              " 'deep',\n",
              " 'learning',\n",
              " 'methods',\n",
              " '.',\n",
              " '[',\n",
              " '24',\n",
              " ']',\n",
              " 'For',\n",
              " 'a',\n",
              " 'language',\n",
              " 'like',\n",
              " 'English',\n",
              " ',',\n",
              " 'this',\n",
              " 'is',\n",
              " 'fairly',\n",
              " 'trivial',\n",
              " ',',\n",
              " 'since',\n",
              " 'words',\n",
              " 'are',\n",
              " 'usually',\n",
              " 'separated',\n",
              " 'by',\n",
              " 'spaces',\n",
              " '.',\n",
              " 'However',\n",
              " ',',\n",
              " 'some',\n",
              " 'written',\n",
              " 'languages',\n",
              " 'like',\n",
              " 'Chinese',\n",
              " ',',\n",
              " 'Japanese',\n",
              " 'and',\n",
              " 'Thai',\n",
              " 'do',\n",
              " 'not',\n",
              " 'mark',\n",
              " 'word',\n",
              " 'boundaries',\n",
              " 'in',\n",
              " 'such',\n",
              " 'a',\n",
              " 'fashion',\n",
              " ',',\n",
              " 'and',\n",
              " 'in',\n",
              " 'those',\n",
              " 'languages',\n",
              " 'text',\n",
              " 'segmentation',\n",
              " 'is',\n",
              " 'a',\n",
              " 'significant',\n",
              " 'task',\n",
              " 'requiring',\n",
              " 'knowledge',\n",
              " 'of',\n",
              " 'the',\n",
              " 'vocabulary',\n",
              " 'and',\n",
              " 'morphology',\n",
              " 'of',\n",
              " 'words',\n",
              " 'in',\n",
              " 'the',\n",
              " 'language',\n",
              " '.',\n",
              " 'Sometimes',\n",
              " 'this',\n",
              " 'process',\n",
              " 'is',\n",
              " 'also',\n",
              " 'used',\n",
              " 'in',\n",
              " 'cases',\n",
              " 'like',\n",
              " 'bag',\n",
              " 'of',\n",
              " 'words',\n",
              " '(',\n",
              " 'BOW',\n",
              " ')',\n",
              " 'creation',\n",
              " 'in',\n",
              " 'data',\n",
              " 'mining',\n",
              " '.',\n",
              " 'Morphological',\n",
              " 'segmentation',\n",
              " ',',\n",
              " 'Separate',\n",
              " 'words',\n",
              " 'into',\n",
              " 'individual',\n",
              " 'morphemes',\n",
              " 'and',\n",
              " 'identify',\n",
              " 'the',\n",
              " 'class',\n",
              " 'of',\n",
              " 'the',\n",
              " 'morphemes',\n",
              " '.',\n",
              " 'The',\n",
              " 'difficulty',\n",
              " 'of',\n",
              " 'this',\n",
              " 'task',\n",
              " 'depends',\n",
              " 'greatly',\n",
              " 'on',\n",
              " 'the',\n",
              " 'complexity',\n",
              " 'of',\n",
              " 'the',\n",
              " 'morphology',\n",
              " '(',\n",
              " 'i.e.',\n",
              " ',',\n",
              " 'the',\n",
              " 'structure',\n",
              " 'of',\n",
              " 'words',\n",
              " ')',\n",
              " 'of',\n",
              " 'the',\n",
              " 'language',\n",
              " 'being',\n",
              " 'considered',\n",
              " '.',\n",
              " 'English',\n",
              " 'has',\n",
              " 'fairly',\n",
              " 'simple',\n",
              " 'morphology',\n",
              " ',',\n",
              " 'especially',\n",
              " 'inflectional',\n",
              " 'morphology',\n",
              " ',',\n",
              " 'and',\n",
              " 'thus',\n",
              " 'it',\n",
              " 'is',\n",
              " 'often',\n",
              " 'possible',\n",
              " 'to',\n",
              " 'ignore',\n",
              " 'this',\n",
              " 'task',\n",
              " 'entirely',\n",
              " 'and',\n",
              " 'simply',\n",
              " 'model',\n",
              " 'all',\n",
              " 'possible',\n",
              " 'forms',\n",
              " 'of',\n",
              " 'a',\n",
              " 'word',\n",
              " '(',\n",
              " 'e.g.',\n",
              " ',',\n",
              " '``',\n",
              " 'open',\n",
              " ',',\n",
              " 'opens',\n",
              " ',',\n",
              " 'opened',\n",
              " ',',\n",
              " 'opening',\n",
              " \"''\",\n",
              " ')',\n",
              " 'as',\n",
              " 'separate',\n",
              " 'words',\n",
              " '.',\n",
              " 'In',\n",
              " 'languages',\n",
              " 'such',\n",
              " 'as',\n",
              " 'Turkish',\n",
              " 'or',\n",
              " 'Meitei',\n",
              " ',',\n",
              " 'a',\n",
              " 'highly',\n",
              " 'agglutinated',\n",
              " 'Indian',\n",
              " 'language',\n",
              " ',',\n",
              " 'however',\n",
              " ',',\n",
              " 'such',\n",
              " 'an',\n",
              " 'approach',\n",
              " 'is',\n",
              " 'not',\n",
              " 'possible',\n",
              " ',',\n",
              " 'as',\n",
              " 'each',\n",
              " 'dictionary',\n",
              " 'entry',\n",
              " 'has',\n",
              " 'thousands',\n",
              " 'of',\n",
              " 'possible',\n",
              " 'word',\n",
              " 'forms',\n",
              " '.',\n",
              " 'Lemmatization',\n",
              " ',',\n",
              " 'The',\n",
              " 'task',\n",
              " 'of',\n",
              " 'removing',\n",
              " 'inflectional',\n",
              " 'endings',\n",
              " 'only',\n",
              " 'and',\n",
              " 'to',\n",
              " 'return',\n",
              " 'the',\n",
              " 'base',\n",
              " 'dictionary',\n",
              " 'form',\n",
              " 'of',\n",
              " 'a',\n",
              " 'word',\n",
              " 'which',\n",
              " 'is',\n",
              " 'also',\n",
              " 'known',\n",
              " 'as',\n",
              " 'a',\n",
              " 'lemma',\n",
              " '.',\n",
              " 'Lemmatization',\n",
              " 'is',\n",
              " 'another',\n",
              " 'technique',\n",
              " 'for',\n",
              " 'reducing',\n",
              " 'words',\n",
              " 'to',\n",
              " 'their',\n",
              " 'normalized',\n",
              " 'form',\n",
              " '.',\n",
              " 'But',\n",
              " 'in',\n",
              " 'this',\n",
              " 'case',\n",
              " ',',\n",
              " 'the',\n",
              " 'transformation',\n",
              " 'actually',\n",
              " 'uses',\n",
              " 'a',\n",
              " 'dictionary',\n",
              " 'to',\n",
              " 'map',\n",
              " 'words',\n",
              " 'to',\n",
              " 'their',\n",
              " 'actual',\n",
              " 'form',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp9Vvi-K2dld",
        "outputId": "c70cf550-214f-4bdc-f27a-3e5e4a89245b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(words)\n"
      ],
      "metadata": {
        "id": "P35T6PV523Y8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RpQlaiO3_-cO",
        "outputId": "29fa12d3-ddc6-4e57-a864-7f27712d10b3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'given sound clip person peopl speak , determin textual represent speech .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyAX66L9ASUh",
        "outputId": "655a0e66-60b5-4af0-aff3-3e4e9099ac7e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [lemmatizer.lemmatize(word) fpr word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(words)"
      ],
      "metadata": {
        "id": "2l3tFOL_AcYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iWlTBcsYBTIy",
        "outputId": "f157c59b-5b03-408e-905c-729e3dfcddb5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'given sound clip person peopl speak , determin textual represent speech .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning the texts\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "ps = PorterStemmer()\n",
        "wordnet = WordNetLemmatizer()\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "corpus = []\n",
        "for i in range(len(sentences)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features= 1500)\n",
        "x = cv.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "5TZaGP6sBboh"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt14UybeE_Hq",
        "outputId": "073b00e2-968b-4777-b10b-02ef5487cc8a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 1, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 1, 0],\n",
              "       [0, 0, 0, ..., 0, 1, 0],\n",
              "       [0, 0, 2, ..., 0, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "corpus =[]\n",
        "for i in range(len(sentences)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf = TfidfVectorizer()\n",
        "x = tf.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "wA3J9y54F80V"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4LbnnLQGwHL",
        "outputId": "4926eedc-2a8e-4c54-c081-24b6849882e5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.10361244,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.12104834,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.16813784,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.38534793, ..., 0.        , 0.14653863,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}